---
title: "Project 1- Experimental Protocol"
authors: "Eva Martín, Laura González"
output: html_notebook
---

```{r}
suppressPackageStartupMessages({
  library(tidyverse)
  library(rsample)    
  library(yardstick)  
  library(recipes)
  library(themis)
})

set.seed(202)
```

First we load the preprocessed train data:

```{r}
# load preprocessed data
load("../artifacts/train.RData")
load("../artifacts/train_bal.RData")
train <- train_bal
if ("class" %in% names(train)) names(train)[names(train) == "class"] <- "y"

cat("Loaded balanced training set:", nrow(train), "rows,", ncol(train), "columns\n")
print(prop.table(table(train$y)))
```

Then we set 5 cross-validation folds.

```{r}
# K-fold CV on train (stratified)
folds <- vfold_cv(train, v = 5, strata = y)
```

Now will define a base preprocessing recipe.

```{r}
# base recipe: normalize numerics
rec_base <- recipe(y ~ ., data = train) 

rec_normalize <- recipe(y ~ ., data = train) %>%
  step_normalize(all_numeric_predictors())
```


Next, we’ll prep the recipes and create clean design matrices for training/testing so models can be fit directly.

prep(): fits the recipe on the training data — it learns all preprocessing parameters (e.g., means and sds for normalization, levels for one-hot encoding).
→ Think of it as “training the preprocessing”.
bake(): applies the already prepared recipe to any dataset (train, validation, or test) using those learned parameters.
→ Think of it as “transforming the data with the learned preprocessing”.

```{r}
# CV for logistic regression using the base recipe 
cv_logit <- function(folds, use_weights = TRUE) {
  # for each split: (1) prepare, (2) fit and predict, (3) compute metrics
  fold_metrics <- purrr::map(folds$splits, function(spl) {
    fd <- prep_fold_data(spl) # (1)
    w  <- if (use_weights) fd$w_tr else NULL
    p  <- fit_predict_logit(fd$X_tr, fd$y_tr, fd$X_va, weights = w) # (2)
    eval_metrics(fd$y_va, p) # (3)
  }) %>% dplyr::bind_rows()

  summary <- dplyr::summarise(
    fold_metrics,
    dplyr::across(
      dplyr::everything(),
      list(mean = mean, sd = sd)
    )
  )

  list(per_fold = fold_metrics, summary = summary)
}

# Helper 1: Prepare
# fold-wise prep: base recipe only + per-fold class weights
prep_fold_data <- function(split) {
  ana <- rsample::analysis(split)
  ass <- rsample::assessment(split)

  # prep the base recipe on analysis data
  rec_prep <- recipes::prep(rec_base, training = ana, retain = TRUE)

  # bake analysis and assessment to design matrices
  ana_bake <- recipes::bake(rec_prep, new_data = ana)
  ass_bake <- recipes::bake(rec_prep, new_data = ass)

  # split into x / y
  y_tr <- factor(ana_bake$y, levels = c("yes","no"))
  X_tr <- dplyr::select(ana_bake, -y)

  y_va <- factor(ass_bake$y, levels = c("yes","no"))
  X_va <- dplyr::select(ass_bake, -y)

  # per-fold class weights (inverse frequency)
  tab <- table(y_tr)
  w_yes <- as.numeric(sum(tab) / (2 * tab["yes"]))
  w_no  <- as.numeric(sum(tab) / (2 * tab["no"]))
  w_tr  <- ifelse(y_tr == "yes", w_yes, w_no)

  list(X_tr = X_tr, y_tr = y_tr, X_va = X_va, y_va = y_va, w_tr = w_tr)
}

# Helper 2: Fit and predict
# fit logistic regression and return validation probabilities
fit_predict_logit <- function(X_tr, y_tr, X_va, weights = NULL) {
  # turn y into 0/1
  y_tr_num <- as.integer(y_tr == "yes")
  df_tr <- cbind.data.frame(y = y_tr_num, X_tr)

  # use per-fold weights if provided
  if (is.null(weights)) {
    fit <- glm(y ~ ., data = df_tr, family = binomial())
  } else {
    stopifnot(length(weights) == nrow(df_tr))
    fit <- glm(y ~ ., data = df_tr, family = binomial(), weights = weights)
  }

  # predict probabilities on validation set
  p_va <- as.numeric(predict(fit, newdata = as.data.frame(X_va), type = "response"))
  p_va
}

# Helper 3: Evaluate metrics
# helper: compute the main metrics from truth and predicted probs
eval_metrics <- function(y_true, p_hat) {
  df <- tibble(
    y = factor(y_true),
    .pred_yes = as.numeric(p_hat),
    .pred_class = factor(ifelse(p_hat >= 0.5, "yes", "no"),
                         levels = levels(y_true))
  )

  # yardstick expects specific column names
  tibble(
    roc_auc       = roc_auc(df, truth = y, .pred_yes)$.estimate,
    pr_auc        = pr_auc(df, truth = y, .pred_yes)$.estimate,
    f1            = f_meas(df, truth = y, estimate = .pred_class)$.estimate,
    bal_accuracy  = bal_accuracy(df, truth = y, estimate = .pred_class)$.estimate,
    accuracy      = accuracy(df, truth = y, estimate = .pred_class)$.estimate
  )
}

v_logit_base <- cv_logit(folds, use_weights = TRUE)
print(v_logit_base$summary)

v_logit_unw  <- cv_logit(folds, use_weights = FALSE)
print(v_logit_unw$summary)
```

Final table:

```{r}
## in case we want to compute confidence intervals:
# k <- nrow(v_logit_base$per_fold)
# 
# summary_with_ci <- v_logit_base$summary %>%
#   mutate(
#     accuracy_lower     = accuracy_mean     - 1.96 * accuracy_sd     / sqrt(k),
#     accuracy_upper     = accuracy_mean     + 1.96 * accuracy_sd     / sqrt(k),
#     f1_lower           = f1_mean           - 1.96 * f1_sd           / sqrt(k),
#     f1_upper           = f1_mean           + 1.96 * f1_sd           / sqrt(k),
#     roc_auc_lower      = roc_auc_mean      - 1.96 * roc_auc_sd      / sqrt(k),
#     roc_auc_upper      = roc_auc_mean      + 1.96 * roc_auc_sd      / sqrt(k),
#     pr_auc_lower       = pr_auc_mean       - 1.96 * pr_auc_sd       / sqrt(k),
#     pr_auc_upper       = pr_auc_mean       + 1.96 * pr_auc_sd       / sqrt(k),
#     bal_acc_lower      = bal_accuracy_mean - 1.96 * bal_accuracy_sd / sqrt(k),
#     bal_acc_upper      = bal_accuracy_mean + 1.96 * bal_accuracy_sd / sqrt(k)
#   )

s1 <- v_logit_base$summary
s2 <- v_logit_unw$summary

digits <- 2
fmt_pm  <- function(m,s) sprintf(paste0("%.",digits,"f ± %.",digits,"f"), m, s)
tbl <- tibble::tibble(
  Method       = c("Logit (weighted)", "Logit (unweighted)"),
  Accuracy     = c(fmt_pm(s1$accuracy_mean, s1$accuracy_sd),
                   fmt_pm(s2$accuracy_mean, s2$accuracy_sd)),
  `F1-score`   = c(fmt_pm(s1$f1_mean, s1$f1_sd),
                   fmt_pm(s2$f1_mean, s2$f1_sd)),
  AUC          = c(fmt_pm(s1$roc_auc_mean, s1$roc_auc_sd),
                   fmt_pm(s2$roc_auc_mean, s2$roc_auc_sd)),
  `Training time (s)` = c("", "")   # pon tiempos si los tienes, p.ej. c("15.3","127.8")
)
tbl
```
LASSO Regularized Logistic Regression with the same cross validation
```{r}


library(glmnet)
library(yardstick)
library(tibble)

cv_lasso_custom <- function(folds) {
  fold_metrics <- purrr::map(folds$splits, function(spl) {
    ana <- rsample::analysis(spl)
    ass <- rsample::assessment(spl)
    
    # matrices
    x_tr <- model.matrix(y ~ ., data = ana)[, -1]
    y_tr <- as.numeric(ana$y == "yes")
    x_va <- model.matrix(y ~ ., data = ass)[, -1]
    y_va <- factor(ass$y, levels = c("yes","no"))
    
    # entrenamiento con CV interno (10-fold) para lambda
    cvfit <- cv.glmnet(x_tr, y_tr, family = "binomial", alpha = 1, nfolds = 10)
    p_va <- as.numeric(predict(cvfit, newx = x_va, s = "lambda.min", type = "response"))
    
    df <- tibble(
      y = y_va,
      .pred_yes = p_va,
      .pred_class = factor(ifelse(p_va >= 0.5, "yes", "no"), levels = c("yes","no"))
    )
    
    tibble(
      roc_auc = roc_auc(df, truth = y, .pred_yes)$.estimate,
      f1 = f_meas(df, truth = y, estimate = .pred_class)$.estimate,
      accuracy = accuracy(df, truth = y, estimate = .pred_class)$.estimate
    )
  }) %>% bind_rows()
  
  summary <- summarise(fold_metrics, across(everything(), list(mean = mean, sd = sd)))
  list(per_fold = fold_metrics, summary = summary)
}

v_lasso <- cv_lasso_custom(folds)
print(v_lasso$summary)


```

```{r}
s1 <- v_logit_base$summary
s2 <- v_logit_unw$summary
s3 <- v_lasso$summary

digits <- 3
fmt_pm  <- function(m,s) sprintf(paste0("%.",digits,"f ± %.",digits,"f"), m, s)

tbl_glm_all <- tibble::tibble(
  Method = c("GLM (weighted)", "GLM (unweighted)", "GLM (LASSO)"),
  Accuracy = c(fmt_pm(s1$accuracy_mean, s1$accuracy_sd),
               fmt_pm(s2$accuracy_mean, s2$accuracy_sd),
               fmt_pm(s3$accuracy_mean, s3$accuracy_sd)),
  `F1-score` = c(fmt_pm(s1$f1_mean, s1$f1_sd),
                 fmt_pm(s2$f1_mean, s2$f1_sd),
                 fmt_pm(s3$f1_mean, s3$f1_sd)),
  `ROC AUC` = c(fmt_pm(s1$roc_auc_mean, s1$roc_auc_sd),
                fmt_pm(s2$roc_auc_mean, s2$roc_auc_sd),
                fmt_pm(s3$roc_auc_mean, s3$roc_auc_sd))
)
tbl_glm_all

```

Linear Discriminant Analysis (LDA)

```{r}

library(MASS)

cv_lda <- function(folds) {
fold_metrics <- purrr::map(folds$splits, function(spl) {
ana <- rsample::analysis(spl)
ass <- rsample::assessment(spl)

# Preprocessing same as GLM
rec_prep <- recipes::prep(rec_normalize, training = ana, retain = TRUE)
ana_bake <- recipes::bake(rec_prep, new_data = ana)
ass_bake <- recipes::bake(rec_prep, new_data = ass)

y_tr <- factor(ana_bake$y, levels = c("yes","no"))
X_tr <- dplyr::select(ana_bake, -y)
y_va <- factor(ass_bake$y, levels = c("yes","no"))
X_va <- dplyr::select(ass_bake, -y)

# Training
lda_fit <- lda(y_tr ~ ., data = cbind(y_tr, X_tr))
pred <- predict(lda_fit, newdata = X_va)

# Predictions
p_va <- as.numeric(pred$posterior[, "yes"])
class_pred <- factor(pred$class, levels = c("yes","no"))

#  yardstick evaluation
df <- tibble(
  y = y_va,
  .pred_yes = p_va,
  .pred_class = class_pred
)

tibble(
  roc_auc = roc_auc(df, truth = y, .pred_yes)$.estimate,
  pr_auc = pr_auc(df, truth = y, .pred_yes)$.estimate,
  f1 = f_meas(df, truth = y, estimate = .pred_class)$.estimate,
  bal_accuracy = bal_accuracy(df, truth = y, estimate = .pred_class)$.estimate,
  accuracy = accuracy(df, truth = y, estimate = .pred_class)$.estimate
)

}) %>% bind_rows()

summary <- summarise(fold_metrics, across(everything(), list(mean = mean, sd = sd)))
list(per_fold = fold_metrics, summary = summary)
}

#CV for LDA

v_lda <- cv_lda(folds)
print(v_lda$summary)

```

QDA
```{r}
library(MASS)

cv_qda <- function(folds) {
  fold_metrics <- purrr::map(folds$splits, function(spl) {
    ana <- rsample::analysis(spl)
    ass <- rsample::assessment(spl)

    # Preprocessing same as LDA y GLM
    rec_prep <- recipes::prep(rec_normalize, training = ana, retain = TRUE)
    ana_bake <- recipes::bake(rec_prep, new_data = ana)
    ass_bake <- recipes::bake(rec_prep, new_data = ass)

    y_tr <- factor(ana_bake$y, levels = c("yes","no"))
    X_tr <- dplyr::select(ana_bake, -y)
    y_va <- factor(ass_bake$y, levels = c("yes","no"))
    X_va <- dplyr::select(ass_bake, -y)

    # Training QDA
    qda_fit <- qda(y_tr ~ ., data = cbind(y_tr, X_tr))
    pred <- predict(qda_fit, newdata = X_va)

    # Predictions
    p_va <- as.numeric(pred$posterior[, "yes"])
    class_pred <- factor(pred$class, levels = c("yes","no"))

    # Evaluation with yardstick
    df <- tibble(
      y = y_va,
      .pred_yes = p_va,
      .pred_class = class_pred
    )

    tibble(
      roc_auc = roc_auc(df, truth = y, .pred_yes)$.estimate,
      pr_auc = pr_auc(df, truth = y, .pred_yes)$.estimate,
      f1 = f_meas(df, truth = y, estimate = .pred_class)$.estimate,
      bal_accuracy = bal_accuracy(df, truth = y, estimate = .pred_class)$.estimate,
      accuracy = accuracy(df, truth = y, estimate = .pred_class)$.estimate
    )
  }) %>% bind_rows()

  summary <- summarise(fold_metrics, across(everything(), list(mean = mean, sd = sd)))
  list(per_fold = fold_metrics, summary = summary)
}

# CV for QDA
v_qda <- cv_qda(folds)
print(v_qda$summary)

```
The Quadratic Discriminant Analysis (QDA) model could not be fitted on our dataset due to rank deficiency in the covariance matrices, which occurs when the number of predictors is large and many of them are linearly dependent (e.g., dummy variables from one-hot encoding). To address this limitation, we adopt Regularized Discriminant Analysis (RDA), a generalization of QDA that introduces terms to stabilize the covariance estimates and interpolate between LDA and QDA.


