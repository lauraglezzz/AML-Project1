---
title: "Project 1- Experimental Protocol"
authors: "Eva Martín, Laura González"
output: html_notebook
---

```{r}
suppressPackageStartupMessages({
  library(tidyverse)
  library(rsample)    
  library(yardstick)  
  library(recipes)
  library(themis)
  library(furrr)
  library(purrr)
})

# parallel plan
# - Windows/macOS: multisession
# - Linux: multicore 
plan(multisession) 

set.seed(202)
```

First we load the preprocessed train data:

```{r}
# load preprocessed data
load("../artifacts/train_bal.RData")
train <- train_bal

cat("Loaded balanced training set:", nrow(train), "rows,", ncol(train), "columns\n")
print(prop.table(table(train$y)))
```

Then we set 5 cross-validation folds.

```{r}
# K-fold CV on train (stratified)
folds <- vfold_cv(train, v = 5, strata = y)
```

Now will define a base preprocessing recipe.

```{r}
# base recipe: normalize numerics
rec_base <- recipe(y ~ ., data = train) 

rec_normalize <- recipe(y ~ ., data = train) %>%
step_integer(all_nominal_predictors(), zero_based = TRUE) %>%  # <-- clave
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())
```


Next, we’ll prep the recipes and create clean design matrices for training/testing so models can be fit directly.

prep(): fits the recipe on the training data — it learns all preprocessing parameters (e.g., means and sds for normalization, levels for one-hot encoding).
→ Think of it as “training the preprocessing”.
bake(): applies the already prepared recipe to any dataset (train, validation, or test) using those learned parameters.
→ Think of it as “transforming the data with the learned preprocessing”.

```{r}
# CV for logistic regression using the base recipe 
cv_logit <- function(folds, use_weights = TRUE) {
  # for each split: (1) prepare, (2) fit and predict, (3) compute metrics
  fold_metrics <- purrr::map(folds$splits, function(spl) {
    fd <- prep_fold_data(spl) # (1)
    w  <- if (use_weights) fd$w_tr else NULL
    p  <- fit_predict_logit(fd$X_tr, fd$y_tr, fd$X_va, weights = w) # (2)
    eval_metrics(fd$y_va, p) # (3)
  }) %>% dplyr::bind_rows()

  summary <- dplyr::summarise(
    fold_metrics,
    dplyr::across(
      dplyr::everything(),
      list(mean = mean, sd = sd)
    )
  )

  list(per_fold = fold_metrics, summary = summary)
}

# Helper 1: Prepare
# fold-wise prep: base recipe only + per-fold class weights
prep_fold_data <- function(split) {
  ana <- rsample::analysis(split)
  ass <- rsample::assessment(split)

  # prep the base recipe on analysis data
  rec_prep <- recipes::prep(rec_base, training = ana, retain = TRUE)

  # bake analysis and assessment to design matrices
  ana_bake <- recipes::bake(rec_prep, new_data = ana)
  ass_bake <- recipes::bake(rec_prep, new_data = ass)

  # split into x / y
  y_tr <- factor(ana_bake$y, levels = c("yes","no"))
  X_tr <- dplyr::select(ana_bake, -y)

  y_va <- factor(ass_bake$y, levels = c("yes","no"))
  X_va <- dplyr::select(ass_bake, -y)

  # per-fold class weights (inverse frequency)
  tab <- table(y_tr)
  w_yes <- as.numeric(sum(tab) / (2 * tab["yes"]))
  w_no  <- as.numeric(sum(tab) / (2 * tab["no"]))
  w_tr  <- ifelse(y_tr == "yes", w_yes, w_no)

  list(X_tr = X_tr, y_tr = y_tr, X_va = X_va, y_va = y_va, w_tr = w_tr)
}

# Helper 2: Fit and predict
# fit logistic regression and return validation probabilities
fit_predict_logit <- function(X_tr, y_tr, X_va, weights = NULL) {
  # turn y into 0/1
  y_tr_num <- as.integer(y_tr == "yes")
  df_tr <- cbind.data.frame(y = y_tr_num, X_tr)

  # use per-fold weights if provided
  if (is.null(weights)) {
    fit <- glm(y ~ ., data = df_tr, family = binomial())
  } else {
    stopifnot(length(weights) == nrow(df_tr))
    fit <- glm(y ~ ., data = df_tr, family = binomial(), weights = weights)
  }

  # predict probabilities on validation set
  p_va <- as.numeric(predict(fit, newdata = as.data.frame(X_va), type = "response"))
  p_va
}

# Helper 3: Evaluate metrics
# helper: compute the main metrics from truth and predicted probs
eval_metrics <- function(y_true, p_hat) {
  df <- tibble(
    y = factor(y_true),
    .pred_yes = as.numeric(p_hat),
    .pred_class = factor(ifelse(p_hat >= 0.5, "yes", "no"),
                         levels = levels(y_true))
  )

  # yardstick expects specific column names
  tibble(
    roc_auc       = roc_auc(df, truth = y, .pred_yes)$.estimate,
    pr_auc        = pr_auc(df, truth = y, .pred_yes)$.estimate,
    f1            = f_meas(df, truth = y, estimate = .pred_class)$.estimate,
    bal_accuracy  = bal_accuracy(df, truth = y, estimate = .pred_class)$.estimate,
    accuracy      = accuracy(df, truth = y, estimate = .pred_class)$.estimate
  )
}

v_logit_base <- cv_logit(folds, use_weights = TRUE)
print(v_logit_base$summary)

v_logit_unw  <- cv_logit(folds, use_weights = FALSE)
print(v_logit_unw$summary)
```

Final table:

```{r}
## in case we want to compute confidence intervals:
# k <- nrow(v_logit_base$per_fold)
# 
# summary_with_ci <- v_logit_base$summary %>%
#   mutate(
#     accuracy_lower     = accuracy_mean     - 1.96 * accuracy_sd     / sqrt(k),
#     accuracy_upper     = accuracy_mean     + 1.96 * accuracy_sd     / sqrt(k),
#     f1_lower           = f1_mean           - 1.96 * f1_sd           / sqrt(k),
#     f1_upper           = f1_mean           + 1.96 * f1_sd           / sqrt(k),
#     roc_auc_lower      = roc_auc_mean      - 1.96 * roc_auc_sd      / sqrt(k),
#     roc_auc_upper      = roc_auc_mean      + 1.96 * roc_auc_sd      / sqrt(k),
#     pr_auc_lower       = pr_auc_mean       - 1.96 * pr_auc_sd       / sqrt(k),
#     pr_auc_upper       = pr_auc_mean       + 1.96 * pr_auc_sd       / sqrt(k),
#     bal_acc_lower      = bal_accuracy_mean - 1.96 * bal_accuracy_sd / sqrt(k),
#     bal_acc_upper      = bal_accuracy_mean + 1.96 * bal_accuracy_sd / sqrt(k)
#   )

s1 <- v_logit_base$summary
s2 <- v_logit_unw$summary

digits <- 2
fmt_pm  <- function(m,s) sprintf(paste0("%.",digits,"f ± %.",digits,"f"), m, s)
tbl <- tibble::tibble(
  Method       = c("Logit (weighted)", "Logit (unweighted)"),
  Accuracy     = c(fmt_pm(s1$accuracy_mean, s1$accuracy_sd),
                   fmt_pm(s2$accuracy_mean, s2$accuracy_sd)),
  `F1-score`   = c(fmt_pm(s1$f1_mean, s1$f1_sd),
                   fmt_pm(s2$f1_mean, s2$f1_sd)),
  AUC          = c(fmt_pm(s1$roc_auc_mean, s1$roc_auc_sd),
                   fmt_pm(s2$roc_auc_mean, s2$roc_auc_sd)),
  `Training time (s)` = c("", "")   # pon tiempos si los tienes, p.ej. c("15.3","127.8")
)
tbl
```
LASSO Regularized Logistic Regression with the same cross validation
```{r}


library(glmnet)
library(yardstick)
library(tibble)

cv_lasso_custom <- function(folds) {
  fold_metrics <- purrr::map(folds$splits, function(spl) {
    ana <- rsample::analysis(spl)
    ass <- rsample::assessment(spl)
    
    # matrices
    x_tr <- model.matrix(y ~ ., data = ana)[, -1]
    y_tr <- as.numeric(ana$y == "yes")
    x_va <- model.matrix(y ~ ., data = ass)[, -1]
    y_va <- factor(ass$y, levels = c("yes","no"))
    
    # entrenamiento con CV interno (10-fold) para lambda
    cvfit <- cv.glmnet(x_tr, y_tr, family = "binomial", alpha = 1, nfolds = 10)
    p_va <- as.numeric(predict(cvfit, newx = x_va, s = "lambda.min", type = "response"))
    
    df <- tibble(
      y = y_va,
      .pred_yes = p_va,
      .pred_class = factor(ifelse(p_va >= 0.5, "yes", "no"), levels = c("yes","no"))
    )
    
    tibble(
      roc_auc = roc_auc(df, truth = y, .pred_yes)$.estimate,
      f1 = f_meas(df, truth = y, estimate = .pred_class)$.estimate,
      accuracy = accuracy(df, truth = y, estimate = .pred_class)$.estimate
    )
  }) %>% bind_rows()
  
  summary <- summarise(fold_metrics, across(everything(), list(mean = mean, sd = sd)))
  list(per_fold = fold_metrics, summary = summary)
}

v_lasso <- cv_lasso_custom(folds)
print(v_lasso$summary)


```

```{r}
s1 <- v_logit_base$summary
s2 <- v_logit_unw$summary
s3 <- v_lasso$summary

digits <- 3
fmt_pm  <- function(m,s) sprintf(paste0("%.",digits,"f ± %.",digits,"f"), m, s)

tbl_glm_all <- tibble::tibble(
  Method = c("GLM (weighted)", "GLM (unweighted)", "GLM (LASSO)"),
  Accuracy = c(fmt_pm(s1$accuracy_mean, s1$accuracy_sd),
               fmt_pm(s2$accuracy_mean, s2$accuracy_sd),
               fmt_pm(s3$accuracy_mean, s3$accuracy_sd)),
  `F1-score` = c(fmt_pm(s1$f1_mean, s1$f1_sd),
                 fmt_pm(s2$f1_mean, s2$f1_sd),
                 fmt_pm(s3$f1_mean, s3$f1_sd)),
  `ROC AUC` = c(fmt_pm(s1$roc_auc_mean, s1$roc_auc_sd),
                fmt_pm(s2$roc_auc_mean, s2$roc_auc_sd),
                fmt_pm(s3$roc_auc_mean, s3$roc_auc_sd))
)
tbl_glm_all

```

Linear Discriminant Analysis (LDA)

```{r}

library(MASS)

cv_lda <- function(folds) {
fold_metrics <- purrr::map(folds$splits, function(spl) {
ana <- rsample::analysis(spl)
ass <- rsample::assessment(spl)

# Preprocessing same as GLM
rec_prep <- recipes::prep(rec_normalize, training = ana, retain = TRUE)
ana_bake <- recipes::bake(rec_prep, new_data = ana)
ass_bake <- recipes::bake(rec_prep, new_data = ass)

y_tr <- factor(ana_bake$y, levels = c("yes","no"))
X_tr <- dplyr::select(ana_bake, -y)
y_va <- factor(ass_bake$y, levels = c("yes","no"))
X_va <- dplyr::select(ass_bake, -y)

# Training
lda_fit <- lda(y_tr ~ ., data = cbind(y_tr, X_tr))
pred <- predict(lda_fit, newdata = X_va)

# Predictions
p_va <- as.numeric(pred$posterior[, "yes"])
class_pred <- factor(pred$class, levels = c("yes","no"))

#  yardstick evaluation
df <- tibble(
  y = y_va,
  .pred_yes = p_va,
  .pred_class = class_pred
)

tibble(
  roc_auc = roc_auc(df, truth = y, .pred_yes)$.estimate,
  pr_auc = pr_auc(df, truth = y, .pred_yes)$.estimate,
  f1 = f_meas(df, truth = y, estimate = .pred_class)$.estimate,
  bal_accuracy = bal_accuracy(df, truth = y, estimate = .pred_class)$.estimate,
  accuracy = accuracy(df, truth = y, estimate = .pred_class)$.estimate
)

}) %>% bind_rows()

summary <- summarise(fold_metrics, across(everything(), list(mean = mean, sd = sd)))
list(per_fold = fold_metrics, summary = summary)
}

#CV for LDA

v_lda <- cv_lda(folds)
print(v_lda$summary)

```

QDA
```{r}
library(MASS)

cv_qda <- function(folds) {
  fold_metrics <- purrr::map(folds$splits, function(spl) {
    ana <- rsample::analysis(spl)
    ass <- rsample::assessment(spl)

    # Preprocessing same as LDA y GLM
    rec_prep <- recipes::prep(rec_normalize, training = ana, retain = TRUE)
    ana_bake <- recipes::bake(rec_prep, new_data = ana)
    ass_bake <- recipes::bake(rec_prep, new_data = ass)

    y_tr <- factor(ana_bake$y, levels = c("yes","no"))
    X_tr <- dplyr::select(ana_bake, -y)
    y_va <- factor(ass_bake$y, levels = c("yes","no"))
    X_va <- dplyr::select(ass_bake, -y)

    # Training QDA
    qda_fit <- qda(y_tr ~ ., data = cbind(y_tr, X_tr))
    pred <- predict(qda_fit, newdata = X_va)

    # Predictions
    p_va <- as.numeric(pred$posterior[, "yes"])
    class_pred <- factor(pred$class, levels = c("yes","no"))

    # Evaluation with yardstick
    df <- tibble(
      y = y_va,
      .pred_yes = p_va,
      .pred_class = class_pred
    )

    tibble(
      roc_auc = roc_auc(df, truth = y, .pred_yes)$.estimate,
      pr_auc = pr_auc(df, truth = y, .pred_yes)$.estimate,
      f1 = f_meas(df, truth = y, estimate = .pred_class)$.estimate,
      bal_accuracy = bal_accuracy(df, truth = y, estimate = .pred_class)$.estimate,
      accuracy = accuracy(df, truth = y, estimate = .pred_class)$.estimate
    )
  }) %>% bind_rows()

  summary <- summarise(fold_metrics, across(everything(), list(mean = mean, sd = sd)))
  list(per_fold = fold_metrics, summary = summary)
}

# CV for QDA
#v_qda <- cv_qda(folds)
#print(v_qda$summary)
```
The Quadratic Discriminant Analysis (QDA) model could not be fitted on our dataset due to rank deficiency in the covariance matrices, which occurs when the number of predictors is large and many of them are linearly dependent (e.g., dummy variables from one-hot encoding). To address this limitation, we adopt Regularized Discriminant Analysis (RDA), a generalization of QDA that introduces terms to stabilize the covariance estimates and interpolate between LDA and QDA.


```{r}
# ===========================
# RDA
# ===========================
cv_rda <- function(folds,
                   gammas  = c(0, .1, .3, .5, .7, 1),
                   lambdas = c(0, .1, .3, .5, .7, 1),
                   lambda_floor = 1e-3) {

  grid <- expand.grid(gamma = gammas, lambda = lambdas)

  eval_one_combo <- function(gamma, lambda) {
    gamma  <- min(max(gamma, 0), 1)
    lambda <- max(min(max(lambda, 0), 1), lambda_floor)

    per_fold <- map_dfr(seq_along(folds$splits), function(i) {
      spl <- folds$splits[[i]]
      ana <- rsample::analysis(spl)
      ass <- rsample::assessment(spl)

      rec_prep <- recipes::prep(rec_normalize, training = ana, retain = TRUE)
      ana_bake <- recipes::bake(rec_prep, new_data = ana)
      ass_bake <- recipes::bake(rec_prep, new_data = ass)

      y_tr <- factor(ana_bake$y, levels = c("yes","no"))
      y_va <- factor(ass_bake$y, levels = c("yes","no"))

      X_tr <- dplyr::select(ana_bake, -y)
      X_va <- dplyr::select(ass_bake, -y)

      # Alinear columnas por si el recipe dropea algo
      common_cols <- intersect(names(X_tr), names(X_va))
      X_tr <- X_tr[, common_cols, drop = FALSE]
      X_va <- X_va[, common_cols, drop = FALSE]

      # Folds degenerados
      if (ncol(X_tr) == 0 || ncol(X_va) == 0 ||
          length(unique(y_tr)) < 2 || length(unique(y_va)) < 2) {
        return(tibble(
          fold = i, roc_auc = NA_real_, pr_auc = NA_real_,
          f1 = NA_real_, bal_accuracy = NA_real_, accuracy = NA_real_
        ))
      }

      # Entrenar (con retry a lambda=1 si falla)
      fit  <- try(klaR::rda(x = as.matrix(X_tr), grouping = y_tr,
                            gamma = gamma, lambda = lambda), silent = TRUE)
      pred <- if (!inherits(fit, "try-error"))
        try(predict(fit, newdata = as.matrix(X_va)), silent = TRUE) else "try-error"

      if (inherits(fit, "try-error") || inherits(pred, "try-error")) {
        fit  <- try(klaR::rda(x = as.matrix(X_tr), grouping = y_tr,
                              gamma = gamma, lambda = 1), silent = TRUE)
        pred <- if (!inherits(fit, "try-error"))
          try(predict(fit, newdata = as.matrix(X_va)), silent = TRUE) else "try-error"
      }

      if (inherits(fit, "try-error") || inherits(pred, "try-error") ||
          is.null(pred$posterior) || !"yes" %in% colnames(pred$posterior)) {
        return(tibble(
          fold = i, roc_auc = NA_real_, pr_auc = NA_real_,
          f1 = NA_real_, bal_accuracy = NA_real_, accuracy = NA_real_
        ))
      }

      df <- tibble(
        y = y_va,
        .pred_yes = as.numeric(pred$posterior[, "yes"]),
        .pred_class = factor(pred$class, levels = c("yes","no"))
      )

      tibble(
        fold = i,
        roc_auc = roc_auc(df, truth = y, .pred_yes)$.estimate,
        pr_auc  = pr_auc(df,  truth = y, .pred_yes)$.estimate,
        f1      = f_meas(df, truth = y, estimate = .pred_class)$.estimate,
        bal_accuracy = bal_accuracy(df, truth = y, estimate = .pred_class)$.estimate,
        accuracy = accuracy(df, truth = y, estimate = .pred_class)$.estimate
      )
    })

    summary <- per_fold %>%
      summarise(across(c(roc_auc, pr_auc, f1, bal_accuracy, accuracy),
                       list(mean = ~{m <- mean(.x, na.rm = TRUE); if (is.nan(m)) NA_real_ else m},
                            sd   = ~sd(.x, na.rm = TRUE)))) %>%
      mutate(gamma = gamma, lambda = lambda, .before = 1)

    list(per_fold = per_fold, summary = summary)
  }

  # PAR: evaluamos la rejilla en paralelo
  res_list <- future_pmap(
  .l = list(grid$gamma, grid$lambda),
  .f = ~ eval_one_combo(..1, ..2),
  .options = furrr::furrr_options(
    seed = TRUE,
    scheduling = 1,
    packages = c("recipes","klaR","yardstick","dplyr","tibble","rsample")
  )
)

  grid_summary <- bind_rows(lapply(res_list, `[[`, "summary")) %>%
    arrange(desc(roc_auc_mean))

  list(grid_summary = grid_summary, results = res_list)
}
rda_out <- cv_rda(folds)
slice(rda_out$grid_summary, 1)
```

To calibrate the Regularized Discriminant Analysis (RDA) model, a cross-validated grid search was performed over a two-dimensional hyperparameter space  ( γ , λ ) (γ,λ), where  γ γ controls the interpolation between LDA and QDA, and  λ λ determines the degree of shrinkage applied to the class-specific covariance matrices. Within each fold, the data were preprocessed using the same recipe applied to all other models (predictor normalization and removal of zero-variance features), ensuring methodological consistency. For every  ( γ , λ ) (γ,λ) combination, an RDA model was trained on the analysis set and evaluated on the assessment set, retrieving posterior probabilities for the positive class. The metrics ROC AUC, PR AUC, F1, balanced accuracy, and accuracy were computed per fold and summarized by their mean and standard deviation. Because certain folds may lead to singular covariance estimates, a fallback mechanism was implemented that automatically retries the model with  λ = 1 λ=1 to ensure numerical stability. Finally, to reduce computational time, the entire grid evaluation was parallelized using future mapping (via the furrr package), allowing each hyperparameter configuration to be processed independently.

### Models comparison

```{r}
# Wilcoxon test: Accuracy

acc_lda <- v_lda$summary$bal_accuracy_mean
acc_rda <- slice(rda_out$grid_summary, 1)$bal_accuracy_mean

wilcox.test(acc_lda, acc_rda, paired = TRUE, alternative = "two.sided")
```

```{r}
# Wilcoxon test: AUC

acc_lda <- v_lda$summary$roc_auc_mean
acc_rda <- slice(rda_out$grid_summary, 1)$roc_auc_mean

wilcox.test(acc_lda, acc_rda, paired = TRUE, alternative = "two.sided")
```
### Other models - Teacher suggestion

```{r}
# --- CV con LIBLINEAR (logistic) manteniendo tu estructura ---
cv_liblinear <- function(folds, type = 0, cost = 1, bias = 1, eps = 1e-5) {
  fold_metrics <- purrr::map(folds$splits, function(spl) {
    fd <- prep_fold_data_liblinear(spl)            # (1) igual que antes pero devolvemos wi
    p  <- fit_predict_liblinear(
      fd$X_tr, fd$y_tr, fd$X_va,
      wi = fd$wi, type = type, cost = cost, bias = bias, eps = eps
    )                                              # (2) ajusta y predice probabilidades
    eval_metrics(fd$y_va, p)                       # (3) métricas (tu helper sirve tal cual)
  }) %>% dplyr::bind_rows()

  summary <- dplyr::summarise(
    fold_metrics,
    dplyr::across(dplyr::everything(), list(mean = mean, sd = sd))
  )

  list(per_fold = fold_metrics, summary = summary)
}

# --- Helper 1: preparación por fold para LIBLINEAR ---
# Igual que el tuyo, pero devolvemos un vector nombrado 'wi' (pesos por clase)
prep_fold_data_liblinear <- function(split) {
  ana <- rsample::analysis(split)
  ass <- rsample::assessment(split)

  rec_prep <- recipes::prep(rec_base, training = ana, retain = TRUE)
  ana_bake <- recipes::bake(rec_prep, new_data = ana)
  ass_bake <- recipes::bake(rec_prep, new_data = ass)

  # y con niveles fijos (positivo primero)
  y_tr <- factor(ana_bake$y, levels = c("yes","no"))
  X_tr <- dplyr::select(ana_bake, -y)

  y_va <- factor(ass_bake$y, levels = c("yes","no"))
  X_va <- dplyr::select(ass_bake, -y)

  # pesos por clase (inversa de la frecuencia). LIBLINEAR espera 'wi' con nombres de clase
  tab <- table(y_tr)
  w_yes <- as.numeric(sum(tab) / (2 * tab["yes"]))
  w_no  <- as.numeric(sum(tab) / (2 * tab["no"]))
  wi    <- c(yes = w_yes, no = w_no)

  list(X_tr = X_tr, y_tr = y_tr, X_va = X_va, y_va = y_va, wi = wi)
}

# --- Helper 2: ajuste y predicción con LIBLINEAR (logistic) ---
# Nota:
# - Necesita matrices numéricas (sin factores); tu recipe debe dejar todo numérico.
# - Probabilidades solo están disponibles para tipos logísticos (p.ej. type = 0, 6 o 7).
fit_predict_liblinear <- function(X_tr, y_tr, X_va, wi = NULL,
                                  type = 0, cost = 1, bias = 1, eps = 1e-5) {
  stopifnot(requireNamespace("LiblineaR", quietly = TRUE))

  # A LIBLINEAR le va bien recibir 'y' como factor o vector numérico con niveles/clases
  # Aseguramos matrices numéricas:
  X_tr_mat <- as.matrix(data.frame(X_tr, check.names = FALSE))
  X_va_mat <- as.matrix(data.frame(X_va, check.names = FALSE))

  # Ajuste (type=0: L2-regularized logistic (primal); también puedes probar 6 o 7)
  fit <- LiblineaR::LiblineaR(
    data = X_tr_mat,
    target = y_tr,
    type = type,
    cost = cost,
    bias = bias,
    wi = wi,
    epsilon = eps,
    verbose = FALSE
  )

  # Predicción con probabilidades
  pred <- predict(fit, newx = X_va_mat, proba = TRUE)

  # 'pred$prob' es una matriz con columnas por clase; cogemos la de "yes"
  prob_mat <- pred$prob
  if (is.null(prob_mat)) {
    stop("Este 'type' de LIBLINEAR no devuelve probabilidades. Usa un tipo logístico (p.ej. 0, 6 o 7).")
  }
  col_yes <- which(colnames(prob_mat) == "yes")
  if (length(col_yes) == 0L) {
    # Por si los nombres vienen como niveles numéricos; mapeamos usando niveles de y_tr
    # Buscamos la columna de la clase positiva (primer nivel de y_tr)
    pos_name <- levels(y_tr)[1]
    col_yes <- which(colnames(prob_mat) == pos_name)
  }
  if (length(col_yes) == 0L) {
    # fallback: si hay 2 columnas, asumimos que la primera corresponde al primer nivel
    col_yes <- 1L
  }

  as.numeric(prob_mat[, col_yes])
}

rec_base <- recipe(y ~ ., data = train) %>%
step_integer(all_nominal_predictors(), zero_based = TRUE) %>%  # <-- clave
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

# CV con LIBLINEAR logistic (type = 0) y pesos por clase
v_liblinear <- cv_liblinear(
  folds,
  type = 0,   # L2-regularized logistic regression (primal)
  cost = 1,   # ~ C de SVM; puedes tunearlo
  bias = 1,
  eps = 1e-5
)
print(v_liblinear$summary)
```


